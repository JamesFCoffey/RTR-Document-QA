{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b45bd4-2b51-46f3-9f24-feb01b787b5c",
   "metadata": {},
   "source": [
    "## Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6201f57-d3f2-41b0-8a60-fa139f1b8b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "!pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6cd2c08-9279-4c6a-a4a6-b2a87483243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally show the above output\n",
    "# output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4773a310-6525-4236-ab3b-7eeabcd871ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatically restart kernel after installs so that the environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca429065-1910-4649-82d5-bd0d03b72f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import string, re, collections\n",
    "from datasets import load_dataset, Dataset\n",
    "from evaluate import load\n",
    "from google.auth import default\n",
    "import pandas as pd\n",
    "import pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import subprocess\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "import yaml\n",
    "\n",
    "# Import config\n",
    "with open(\"./config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Setup Pinecone\n",
    "api_key = config[\"pinecone\"][\"api_key\"]\n",
    "environment = config[\"pinecone\"][\"environment\"]\n",
    "search_index_name = config[\"pinecone\"][\"search_index_name\"]\n",
    "\n",
    "# Setup Vertex AI\n",
    "model_name = config[\"vertex_ai\"][\"model_name\"]\n",
    "project_id = config[\"vertex_ai\"][\"project_id\"]\n",
    "location = config[\"vertex_ai\"][\"location\"]\n",
    "bucket = config[\"vertex_ai\"][\"bucket\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3acf79-ec9c-457a-a0be-c95177db9eb7",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "The data is from the `databricks/databricks-dolly-15k` dataset imported from Hugging Face. Among other tasks, the dataset contains examples for closed question answering, information extraction, and summarization. These tasks are useful for training an LLM to extract information for the FAQ serice. From [Databricks](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm):\n",
    "\n",
    ">[databricks-dolly-15k](https://github.com/databrickslabs/dolly/tree/master/data) contains 15,000 high-quality human-generated prompt / response pairs specifically designed for instruction tuning large language models. Under the licensing terms for databricks-dolly-15k ([Creative Commons Attribution-ShareAlike 3.0 Unported License](https://creativecommons.org/licenses/by-sa/3.0/)), anyone can use, modify, or extend this dataset for any purpose, including commercial applications.\n",
    ">\n",
    ">To the best of our knowledge, this dataset is the first open source, human-generated instruction dataset specifically designed to make large language models exhibit the magical interactivity of ChatGPT. databricks-dolly-15k was authored by more than 5,000 Databricks employees during March and April of 2023. These training records are natural, expressive and designed to represent a wide range of the behaviors, from brainstorming and content generation to information extraction and summarization.\n",
    "\n",
    "I'll split this dataset into train and test datasets. Also, I'll use 300 training examples in line with Google's recommendation of 100+ for extractive QA and 100-500+ for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb96f89-2ef1-4c01-b242-bfe1f39bfcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "\n",
    "# Convert dataset to a DataFrame\n",
    "df_full = pd.DataFrame({\n",
    "    \"instruction\": dataset[\"train\"][\"instruction\"],\n",
    "    \"context\": dataset[\"train\"][\"context\"],\n",
    "    \"output_text\": dataset[\"train\"][\"response\"],\n",
    "    \"category\": dataset[\"train\"][\"category\"],\n",
    "})\n",
    "\n",
    "# Keep only rows with context\n",
    "df_full = df_full[df_full[\"category\"].isin([\"closed_qa\", \"information_extraction\", \"summarization\"])]\n",
    "df_full.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Train / Test split\n",
    "df_train = df_full.sample(frac=0.8, random_state=2023)\n",
    "df_test = df_full.drop(df_train.index)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Add semantically similar alterative phrasing of the queries (\"instruction\")\n",
    "df_test = df_test.truncate(after=249) # I got tired of rephrasing questions\n",
    "alt_questions = pd.read_csv(\"./alt_questions.csv\")\n",
    "df_test[\"alt_questions\"] = alt_questions\n",
    "\n",
    "df_train[\"input_text\"] = \"question: \" + df_train[\"instruction\"] + \" context: \" + df_train[\"context\"]\n",
    "df_test[\"input_text\"] = \"question: \" + df_test[\"instruction\"] + \" context: \" + df_test[\"context\"]\n",
    "\n",
    "# Set number of training examples to 300\n",
    "num_train_examples = 300\n",
    "df_train = df_train[:num_train_examples]\n",
    "\n",
    "# It needs to be in a JSONL format for batch prediction\n",
    "test_json = df_test[\"input_text\"].to_frame().rename(columns={\"input_text\": \"prompt\"}).to_json(orient=\"records\", lines=True)\n",
    "with open(\"./test.jsonl\", \"w\") as f:\n",
    "    f.write(test_json)\n",
    "\n",
    "subprocess.run(f\"gsutil cp ./test.jsonl gs://{bucket}/test.jsonl\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedfc81a-c5d3-48f1-aff3-32118dc9eb6d",
   "metadata": {},
   "source": [
    "## Evaluate the Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ff94e-da36-460e-a080-ec40e235f81b",
   "metadata": {},
   "source": [
    "I'll be using F1 score (higher the better, 0-1) to be evaluating models on the test set. Some definitions:\n",
    "\n",
    "- **True Positive:** Number of shared tokens between the prediction and the correct answer.\n",
    "- **False Positive:** Number of tokens in the predicted sequence, excluding the shared tokens.\n",
    "- **False Negative:** Number of tokens in the correct answer, excluding the shared tokens.\n",
    "- **Precision (P):** True Positive / (True Positive + False Positive)\n",
    "- **Recall (R):** True Positive / (True Positive + False Negative)\n",
    "- **F1 Score** 2PR / (P + R)\n",
    "\n",
    "Also, `temperature` will be `0` and `topK` will be `1` as these tend to work well with the foundational model according to Google. I could set up a hyperparameter search if optimization is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a4b7f1f-f954-461e-b3ec-73ff31b149ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_prediction(model_name):\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "    batch_prediction_job = model.batch_predict(\n",
    "        dataset=[f\"gs://{bucket}/test.jsonl\"],\n",
    "        destination_uri_prefix=f\"gs://{bucket}\",\n",
    "        model_parameters={\n",
    "            \"maxOutputTokens\": \"512\",\n",
    "            \"temperature\": \"0\",\n",
    "            \"topP\": \"0.95\",\n",
    "            \"topK\": \"1\",\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    output_file = batch_prediction_job.output_info.gcs_output_directory + \"/000000000000.jsonl\"\n",
    "    subprocess.run(f\"gsutil cp {output_file} ./response.jsonl\", shell=True)\n",
    "    \n",
    "    response = pd.read_json(\"./response.jsonl\", lines=True).drop(columns=[\"status\"])\n",
    "    response[\"instance\"] = response[\"instance\"].apply(lambda x: x[\"prompt\"])\n",
    "    response[\"predictions\"] = response[\"predictions\"].apply(lambda x: x[0][\"content\"])\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41792059-59ee-4753-9f69-a26889f6900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_response = batch_prediction(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18d943bf-0d9a-4d3f-b5e1-d88e4cfc891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.merge(base_model_response, left_on=\"input_text\", right_on=\"instance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f07b82f-81d0-41a0-99ce-c6eaa456511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived from Official evaluation script for SQuAD version 2.0.\n",
    "# https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
    "    \n",
    "def normalize_text(text):\n",
    "\n",
    "    # lower case the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove punctuation\n",
    "    punctuation = set(string.punctuation)\n",
    "    text = \"\".join(char for char in text if char not in punctuation)\n",
    "\n",
    "    # remove articles\n",
    "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "    text =  re.sub(regex, \" \", text)\n",
    "\n",
    "    # correct whitespace\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "  if not text: return []\n",
    "  return normalize_text(text).split()\n",
    "\n",
    "def f1_score(predicted, ground_truth):\n",
    "    predicted_tokens = tokenize(predicted)\n",
    "    ground_truth_tokens = tokenize(ground_truth)\n",
    "    \n",
    "    common = collections.Counter(ground_truth_tokens) & collections.Counter(predicted_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if len(ground_truth_tokens) == 0 or len(predicted_tokens) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(ground_truth_tokens == predicted_tokens)\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = 1.0 * num_same / len(predicted_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd2f148-8c1b-46e4-a057-9b672ace1494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    250.000000\n",
       "mean       0.425794\n",
       "std        0.246602\n",
       "min        0.000000\n",
       "25%        0.250000\n",
       "50%        0.375273\n",
       "75%        0.594595\n",
       "max        1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores_base = []\n",
    "for i in range(len(df_test)):\n",
    "    f1_scores_base.append(f1_score(df_test[\"predictions\"][i], df_test[\"output_text\"][i]))\n",
    "f1_scores_base = pd.Series(f1_scores_base)\n",
    "f1_scores_base.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6262c29-2cad-41ae-ab75-3498c24b4194",
   "metadata": {},
   "source": [
    "## Create a model tuning job\n",
    "\n",
    "The fine tuning recommendations from Google are 100-500 train steps for extractive QA and 200-1000 train steps for summarization. I'll use 300 train steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9716605a-0106-4078-9478-7dc51d5af715",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "\n",
    "\n",
    "def tuning(\n",
    "    project_id: str,\n",
    "    training_data: pd.DataFrame | str,\n",
    "    train_steps: int = 300,\n",
    "    location: str = \"us-central1\"\n",
    ") -> None:\n",
    "    \"\"\"Tune a new model, based on a prompt-response data.\n",
    "\n",
    "    \"training_data\" can be either the GCS URI of a file formatted in JSONL format\n",
    "    (for example: training_data=f'gs://{bucket}/{filename}.jsonl'), or a pandas\n",
    "    DataFrame. Each training example should be JSONL record with two keys, for\n",
    "    example:\n",
    "      {\n",
    "        \"input_text\": <input prompt>,\n",
    "        \"output_text\": <associated output>\n",
    "      },\n",
    "    or the pandas DataFame should contain two columns:\n",
    "      ['input_text', 'output_text']\n",
    "    with rows for each training example.\n",
    "\n",
    "    Args:\n",
    "      project_id: GCP Project ID, used to initialize vertexai\n",
    "      location: GCP Region, used to initialize vertexai\n",
    "      training_data: GCS URI of jsonl file or pandas dataframe of training data\n",
    "      train_steps: Number of training steps to use when tuning the model.\n",
    "    \"\"\"\n",
    "    vertexai.init(project=project_id, location=location, credentials=credentials)\n",
    "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "\n",
    "    model.tune_model(\n",
    "        training_data=training_data,\n",
    "        train_steps=train_steps,\n",
    "        tuning_job_location=location,\n",
    "        tuned_model_location=location,\n",
    "    )\n",
    "\n",
    "    print(model._job.status)\n",
    "\n",
    "tuning(project_id, training_data=df_train, train_steps=300, location=location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd62fb2-067a-4583-8003-277263fd387e",
   "metadata": {},
   "source": [
    "## Evaluate the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc602a-654b-4f75-90b4-69f3ea495728",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model_response = batch_prediction(\"###\")\n",
    "\n",
    "# Rename for merge below\n",
    "df_test.rename(columns={\"predictions\": \"predictions_base\", \"instance\": \"instance_base\"}, inplace=True)\n",
    "df_test = df_test.merge(tuned_model_response, left_on=\"input_text\", right_on=\"instance\")\n",
    "\n",
    "# Get F1 scores\n",
    "f1_scores_tuned = []\n",
    "for i in range(len(df_test)):\n",
    "    f1_scores_tuned.append(f1_score(df_test[\"predictions\"][i], df_test[\"output_text\"][i]))\n",
    "f1_scores_tuned = pd.Series(f1_scores_tuned)\n",
    "f1_scores_tuned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7efc3e-4dc2-47f8-aca5-d60b0e04b16d",
   "metadata": {},
   "source": [
    "## Connect to prediction endpoint of fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8891f58-eec0-457f-97be-afcf0160e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_prediction(\n",
    "    input_text: str,\n",
    "    model_name: str\n",
    ") -> str:\n",
    "\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    parameters = {\n",
    "        \"temperature\": 0,  # Temperature controls the degree of randomness in token selection.\n",
    "        \"max_output_tokens\": 512,  # Token limit determines the maximum amount of text output.\n",
    "        \"top_p\": 0.95,  # Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\n",
    "        \"top_k\": 1,  # A top_k of 1 means the selected token is the most probable among all tokens.\n",
    "    }\n",
    "\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "    response = model.predict(\n",
    "        input_text,\n",
    "        **parameters,\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095d744-0f16-4fe1-9d41-72d0327f0dd9",
   "metadata": {},
   "source": [
    "## Convert the questions to embeddings and store in Pinecone vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf3c4587-4604-4754-9c82-abec30c0ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_index(df, model, api_key, environment, search_index_name, batch_size = 100):\n",
    "    \n",
    "    # Get api_key and environment for that api_key from app.pinecone.io\n",
    "    pinecone.init(api_key=api_key, environment=environment)\n",
    "    \n",
    "    # If the search index doesn't exist, create one\n",
    "    if search_index_name not in pinecone.list_indexes():\n",
    "        pinecone.create_index(\n",
    "            name = search_index_name,\n",
    "            dimension = model.get_sentence_embedding_dimension(),\n",
    "            metric = \"cosine\"\n",
    "        )\n",
    "    search_index = pinecone.Index(search_index_name)\n",
    "    \n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=\"Corpus Progress\"):\n",
    "        # Get the last index of the batch\n",
    "        i_end = min(i+batch_size, len(df))\n",
    "        \n",
    "        questions = df[\"instruction\"][i:i_end].tolist()\n",
    "        \n",
    "        ids = [f\"{x}\" for x in range(i, i_end)]\n",
    "        embeddings = model.encode(questions).tolist()\n",
    "        metadata = [{\"instruction\": df[\"instruction\"][x], \"context\": df[\"context\"][x]} for x in range(i, i_end)]\n",
    "\n",
    "        # Create batch\n",
    "        batch = zip(ids, embeddings, metadata)\n",
    "\n",
    "        # Upsert the batch to the search index\n",
    "        search_index.upsert(vectors = list(batch))\n",
    "    \n",
    "    return search_index        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b29ea70f-ebbb-4151-a22d-536f32d27a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Progress: 100%|██████████| 45/45 [00:20<00:00,  2.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.04467,\n",
       " 'namespaces': {'': {'vector_count': 4467}},\n",
       " 'total_vector_count': 4467}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device != \"cuda\":\n",
    "    print(f\"You are using {device}. Embedding will take a long time unless using a CUDA-enabled GPU.\")\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "search_index = create_search_index(df_full, model, api_key=api_key, environment=environment, search_index_name=search_index_name)\n",
    "\n",
    "# How many embeddings do we have?\n",
    "search_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1304e26-66a8-42a0-a1a4-7127b48bb283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95: When was Tomoaki Komorida born?\n"
     ]
    }
   ],
   "source": [
    "query = df_test[\"alt_questions\"][0]\n",
    "embedded_query = model.encode(query).tolist()\n",
    "query_results = search_index.query(embedded_query, top_k=1, include_metadata=True)\n",
    "for result in query_results[\"matches\"]:\n",
    "    print(f\"{round(result['score'], 2)}: {result['metadata']['instruction']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa83561-b4be-4008-8c38-9f4e60a08ae1",
   "metadata": {},
   "source": [
    "## Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1d9f170-40fa-4bcd-86dd-dd3faa25cb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is July 10, 1981.\n",
      "\n",
      "The question is \"When was Tomoaki Komorida born?\". The context is \"Komorida was born in Kumamoto Prefecture on July 10, 1981\". So the answer is July 10, 1981.\n"
     ]
    }
   ],
   "source": [
    "def rtr_qa(query):\n",
    "    embedded_query = model.encode(query).tolist()\n",
    "    query_results = search_index.query(embedded_query, top_k=1, include_metadata=True)\n",
    "    instruction = query_results[\"matches\"][0][\"metadata\"][\"instruction\"]\n",
    "    context = query_results[\"matches\"][0][\"metadata\"][\"context\"]\n",
    "    input_text = \"question: \" + instruction + \" context: \" + context\n",
    "    response = single_prediction(input_text, model_name)\n",
    "    return response\n",
    "\n",
    "print(rtr_qa(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ea8b1-d791-4d63-b26d-7a1ec881d5f2",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "Before moving on to use PaLM 2, I first fine-tuning a Llama-2 model using the `trl` library from Huggging Face on an Nvidia L4 GPU using QLoRA 4bit quantization. It took a while and it became obvious that I would need to use distributed training. Since I was already on GCP's Vertex AI Workbench, I decided to switch from Hugging Face and not interrupt my development speed.\n",
    "\n",
    "I'll return to this model later, and use the Hugging Face [Accelerate](https://huggingface.co/docs/accelerate/v0.21.0/en/basic_tutorials/notebook) library.\n",
    "\n",
    "I keep the code below for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61730dc-7e13-4b8c-a031-f804cfd58b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture output\n",
    "#!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('###')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3398727-a1d4-4c6a-8f7f-07a488057d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally show the above output\n",
    "# output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513110e-0cf6-49c5-a75e-a4ed8c935dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Automatically restart kernel after installs so that the environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470d1e34-c931-4275-b640-fb92b6644622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset, Dataset\n",
    "# import pandas as pd\n",
    "# from huggingface_hub import notebook_login\n",
    "# from peft import LoraConfig\n",
    "# from transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n",
    "# from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821fabe4-3a5b-4e9e-a5f0-0a59dfd55322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/jupyter/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-1a24287182230a5f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c249bcfe8434a9eaa58b0b52989a90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Load dataset\n",
    "# dataset = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "\n",
    "# # Convert dataset to a DataFrame\n",
    "# df_full = pd.DataFrame({\n",
    "#     \"instruction\": dataset[\"train\"][\"instruction\"],\n",
    "#     \"context\": dataset[\"train\"][\"context\"],\n",
    "#     \"response\": dataset[\"train\"][\"response\"],\n",
    "#     \"category\": dataset[\"train\"][\"category\"],\n",
    "# })\n",
    "\n",
    "# # Keep only rows with context\n",
    "# df_full = df_full[df_full[\"category\"].isin([\"closed_qa\", \"information_extraction\", \"summarization\"])]\n",
    "\n",
    "# # Train / Test split\n",
    "# df_train = df_full.sample(frac=0.8, random_state=2023)\n",
    "# df_test = df_full.drop(df_train.index)\n",
    "# df_train.reset_index(drop=True, inplace=True)\n",
    "# df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Add semantically similar alterative phrasing of the queries (\"instruction\")\n",
    "# df_test = df_test.truncate(after=249) # I got tired of rephrasing questions\n",
    "# alt_questions = pd.read_csv(\"./alt_questions.csv\")\n",
    "# df_test[\"alt_questions\"] = alt_questions\n",
    "\n",
    "# df_train[\"text\"] = \"### Human: \" + df_train[\"context\"] + \"Based on above context, answer the following question: \" + df_train[\"instruction\"] + \"### Assistant: \" + df_train[\"response\"]\n",
    "# df_train = df_train[:64] # Full training would take 8 hrs on Nvidia L4, this takes about 11 minutes\n",
    "\n",
    "# ds_train = Dataset.from_pandas(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47925743-b360-492a-ad45-825570f41ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43aba3cc1ec94b2daff451c2c4d6c254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fd7676c-161c-4fe0-a592-20ffa94dcf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: On the team's first full day at the Olympics in Atlanta, the media announced that O'Neal would join the Los Angeles Lakers on a seven-year, $121 million contract. O'Neal insisted he did not choose Los Angeles for the money; discussing the signing he referred to a couple of his product endorsements, saying: \"I'm tired of hearing about money, money, money, money, money. I just want to play the game, drink Pepsi, wear Reebok.\" The Lakers won 56 games during the 1996–97 season. O'Neal averaged 26.2 points and 12.5 rebounds in his first season with Los Angeles; however, he again missed over 30 games due to injury. The Lakers made the playoffs, but were eliminated in the second round by the Utah Jazz in five games. In his first playoff game for the Lakers, O'Neal scored 46 points against the Portland Trail Blazers, the most for the Lakers in a playoff game since Jerry West had 53 in 1969. On December 17, 1996, O'Neal shoved Dennis Rodman of the Chicago Bulls; Rodman's teammates Scottie Pippen and Michael Jordan restrained Rodman and prevented further conflict. The Los Angeles Daily News reported that O'Neal was willing to be suspended for fighting Rodman, and O'Neal said: \"It's one thing to talk tough and one thing to be tough.\"\n",
      "\n",
      "The following season, O'Neal averaged 28.3 points and 11.4 rebounds. He led the league with a 58.4 field goal percentage, the first of five consecutive seasons in which he did so. The Lakers finished the season 61–21, first in the Pacific Division, and were the second seed in the western conference during the 1998 NBA Playoffs. After defeating the Portland Trail Blazers and Seattle SuperSonics in the first two rounds, the Lakers again fell to the Jazz, this time in a 4–0 sweep.Based on above context, answer the following question: How many seasons did Shaq have over 30 ppg### Assistant: Shaquille O'Neal had three seasons over 30 points per game, all of which were with the Los Angeles Lakers. He had his first season with the Lakers in 1995-96, when he averaged 30.4 points per game. He had his second season in 1996-97, when he averaged 29.7 points per game. He had his final season with the Lakers in 2000-01, when he averaged 31.2 points per game.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline\n",
    "# testing=\"### Human: \" + df_train[\"context\"][0] + \"Based on above context, answer the following question: \" + df_train[\"instruction\"][0] + \"### Assistant: \" + df_train[\"response\"][0]\n",
    "# print(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0493199f-88c3-447d-807c-45dffe08bcab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcb9199cc43488aad48edb08db7dfe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "/home/james_francis_coffey/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# pipe = pipeline(model=model_name, device_map=device_map, model_kwargs={\"load_in_4bit\": True})\n",
    "# output = pipe(testing, do_sample=True, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0040a15-3049-4e42-bd90-6a32d7ab3b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the team's first full day at the Olympics in Atlanta, the media announced that O'Neal would join the Los Angeles Lakers on a seven-year, $121 million contract. O'Neal insisted he did not choose Los Angeles for the money; discussing the signing he referred to a couple of his product endorsements, saying: \"I'm tired of hearing about money, money, money, money, money. I just want to play the game, drink Pepsi, wear Reebok.\" The Lakers won 56 games during the 1996–97 season. O'Neal averaged 26.2 points and 12.5 rebounds in his first season with Los Angeles; however, he again missed over 30 games due to injury. The Lakers made the playoffs, but were eliminated in the second round by the Utah Jazz in five games. In his first playoff game for the Lakers, O'Neal scored 46 points against the Portland Trail Blazers, the most for the Lakers in a playoff game since Jerry West had 53 in 1969. On December 17, 1996, O'Neal shoved Dennis Rodman of the Chicago Bulls; Rodman's teammates Scottie Pippen and Michael Jordan restrained Rodman and prevented further conflict. The Los Angeles Daily News reported that O'Neal was willing to be suspended for fighting Rodman, and O'Neal said: \"It's one thing to talk tough and one thing to be tough.\"\n",
      "\n",
      "The following season, O'Neal averaged 28.3 points and 11.4 rebounds. He led the league with a 58.4 field goal percentage, the first of five consecutive seasons in which he did so. The Lakers finished the season 61–21, first in the Pacific Division, and were the second seed in the western conference during the 1998 NBA Playoffs. After defeating the Portland Trail Blazers and Seattle SuperSonics in the first two rounds, the Lakers again fell to the Jazz, this time in a 4–0 sweep.\n",
      "\n",
      "Based on above context, answer the following question: How many seasons did Shaq have over 30 ppg in his career?\n",
      "\n",
      "\\begin{blockquote}\n",
      "\n",
      "\\strong{Solution:}\n",
      "\\end{blockquote}\n",
      "\n",
      "\\begin{blockquote}\n",
      "\n",
      "\\strong{Total seasons:} 5\n",
      "\n",
      "\\strong{Seasons with 30 ppg:} 1\n",
      "\\end{blockquote}\n",
      "\n",
      "Comment: I would change the third paragraph to \"The following season, O'Neal averaged 28.3 points and 11.4 rebounds. He led the league with a 58.4 field goal percentage, the first of five consecutive seasons in which he did so.\"\n",
      "\n",
      "Comment: @JoeZ. Yeah, you're right. I'll fix it.\n",
      "\n",
      "Comment: @JoeZ. I've changed the 3rd paragraph.\n"
     ]
    }
   ],
   "source": [
    "# print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "409efb11-715f-40b9-954c-f1b8e04c173f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Human: On the team's first full day at the Olympics in Atlanta, the media announced that O'Neal would join the Los Angeles Lakers on a seven-year, $121 million contract. O'Neal insisted he did not choose Los Angeles for the money; discussing the signing he referred to a couple of his product endorsements, saying: \"I'm tired of hearing about money, money, money, money, money. I just want to play the game, drink Pepsi, wear Reebok.\" The Lakers won 56 games during the 1996–97 season. O'Neal averaged 26.2 points and 12.5 rebounds in his first season with Los Angeles; however, he again missed over 30 games due to injury. The Lakers made the playoffs, but were eliminated in the second round by the Utah Jazz in five games. In his first playoff game for the Lakers, O'Neal scored 46 points against the Portland Trail Blazers, the most for the Lakers in a playoff game since Jerry West had 53 in 1969. On December 17, 1996, O'Neal shoved Dennis Rodman of the Chicago Bulls; Rodman's teammates Scottie Pippen and Michael Jordan restrained Rodman and prevented further conflict. The Los Angeles Daily News reported that O'Neal was willing to be suspended for fighting Rodman, and O'Neal said: \"It's one thing to talk tough and one thing to be tough.\"\n",
      "\n",
      "The following season, O'Neal averaged 28.3 points and 11.4 rebounds. He led the league with a 58.4 field goal percentage, the first of five consecutive seasons in which he did so. The Lakers finished the season 61–21, first in the Pacific Division, and were the second seed in the western conference during the 1998 NBA Playoffs. After defeating the Portland Trail Blazers and Seattle SuperSonics in the first two rounds, the Lakers again fell to the Jazz, this time in a 4–0 sweep.Based on above context, answer the following question: How many seasons did Shaq have over 30 ppg### Assistant: Shaquille O'Neal had three seasons over 30 points per game, all of which were with the Los Angeles Lakers. He had his first season with the Lakers in 1995-96, when he averaged 30.4 points per game. He had his second season in 1996-97, when he averaged 29.7 points per game. He had his final season with the Lakers in 2000-01, when he averaged 31.2 points per game.### Human: The Lakers won 56 games during the 1996–97 season. O'Neal averaged 26.2 points and 12.5 rebounds in his first season with Los Angeles; however, he again missed over 30 games due to injury. The Lakers made the playoffs, but were eliminated in the second round by the Utah Jazz in five games. In his first playoff game for the Lakers, O'Neal scored 46 points against the Portland Trail Blazers, the most for the Lakers in a playoff game since Jerry West had 53 in 1969. On December 17, 1996, O'Neal shoved Dennis Rodman of the Chicago Bulls; Rodman's teammates Scottie Pippen and Michael Jordan restrained Rodman and prevented further conflict. The Los Angeles Daily News reported that O'Neal was willing to be suspended for fighting Rodman, and O'Neal said: \"It's one thing to talk tough and one thing to be tough.\"\n",
      "\n",
      "The following season, O'Neal averaged 28.3 points and 11.4 rebounds. He led the league with a 58.4 field goal percentage, the first of five consecutive seasons in which he did so. The Lakers finished the season 61–21, first in the Pacific Division, and were the second seed in the western conference during the 1998 NBA Playoffs. After defeating the Portland Trail Blazers and Seattle SuperSonics in the first two rounds, the Lakers again fell to the Jazz, this time in a 4–0 sweep.\n"
     ]
    }
   ],
   "source": [
    "# from peft import PeftConfig, PeftModel\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# fine_tuned_model = PeftModel.from_pretrained(model, output_dir)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# inputs = tokenizer.encode(testing, return_tensors=\"pt\").to(\"cuda\")\n",
    "# outputs = model.generate(inputs, max_length= 1024)\n",
    "# print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e2d66b-f2a4-4f42-aa4d-57bcf2ba4b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james_francis_coffey/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1795500f1516429495de38c8f8e59af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james_francis_coffey/.local/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454a7943c2774d12b7204ad9f9b04bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/64 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james_francis_coffey/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 09:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.898100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.627300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.922700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.963500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.024900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.615600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.757600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.308300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.749000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.648800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.464300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.790300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.553000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.454600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.578000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.556600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.461200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.705600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.645200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.637300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.682100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.249800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.255300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.269200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.727300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.593300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.594800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.653500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.614000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.445200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.750700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.313700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.281500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.609100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.315100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.808200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.312200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.080300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.569200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.755100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.109600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.183400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.251600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# modified from https://github.com/huggingface/trl/tree/main/examples/scripts/sft_trainer.py\n",
    "\n",
    "# This is a previous experiment where I use PEFT to fine-tune Llama-2-7b on Hugging Face\n",
    "\n",
    "\n",
    "# # Set variables\n",
    "# model_name=\"meta-llama/Llama-2-7b-hf\"\n",
    "# dataset_text_field = \"text\" # The text field of the dataset\n",
    "# log_with = None # use 'wandb' to log with wandb\n",
    "# learning_rate = 2e-4\n",
    "# batch_size = 2\n",
    "# seq_length = 1024 # Input sequence length\n",
    "# gradient_accumulation_steps = 2\n",
    "# load_in_4bit = True # Load the model in 4 bits precision\n",
    "# trust_remote_code = True\n",
    "# output_dir = \"trained_model\"\n",
    "# peft_lora_r = 64 # The r parameter of the LoRA adapters\n",
    "# peft_lora_alpha = 16 # The alpha parameter of the LoRA adapters\n",
    "# logging_steps = 1\n",
    "# use_auth_token = True # Use HuggingFace auth token to access the model\n",
    "# num_train_epochs = 3\n",
    "# max_steps = -1 # The number of training steps\n",
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=load_in_4bit)\n",
    "# device_map = {\"\": 0} # Fit the entire model on the GPU:0\n",
    "# torch_dtype = torch.bfloat16\n",
    "\n",
    "# # Load the model\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=quantization_config,\n",
    "#     device_map=device_map,\n",
    "#     trust_remote_code=trust_remote_code,\n",
    "#     torch_dtype=torch_dtype,\n",
    "#     use_auth_token=use_auth_token\n",
    "# )\n",
    "\n",
    "# # Load the dataset\n",
    "# dataset = ds_train\n",
    "\n",
    "# # Define the training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#     learning_rate=learning_rate,\n",
    "#     logging_steps=logging_steps,\n",
    "#     num_train_epochs=num_train_epochs,\n",
    "#     max_steps=max_steps,\n",
    "#     report_to=log_with,\n",
    "# )\n",
    "\n",
    "# # Define the LoRA configuration\n",
    "# peft_config = LoraConfig(\n",
    "#     r=peft_lora_r,\n",
    "#     lora_alpha=peft_lora_alpha,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# # Define the trainer\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     max_seq_length=seq_length,\n",
    "#     train_dataset=ds_train,\n",
    "#     dataset_text_field=dataset_text_field,\n",
    "#     peft_config=peft_config,\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# # Save the model\n",
    "# trainer.save_model(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch (Local)",
   "language": "python",
   "name": "local-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
